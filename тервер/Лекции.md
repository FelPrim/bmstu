[[Облакова Татьяна Васильевна]]

18/11/2025
Среднее время работы критерия Вальда
$$L_{j}=\prod_{k=1}^j \frac{J(x_{k},D_{1})}{J(x_{k},D_{0})} $$
$$\operatorname{ Ln } L_{j}=\sum _{k=1}^j \ln\left( \frac{J(x_{k},D_{1})}{J(x_{k},D_{0})} \right)=\sum _{k=1}^j y_{k}$$
$$\begin{gather}
M\ln L_{\nu}=\left.M\left(\left. M( L_{\nu} \right\rvert \nu =j) \right) \right\rvert _{j=\nu} =M\left. \left(M\sum _{k=1}^j y_{k}\right) \right\rvert _{j=\nu}=M(\left. j\cdot MX_{k} )\right\rvert _{j=\nu} \\
M\ln L_{\nu}=M\nu\cdot MY_k \\
M_{i}\nu=\frac{M_{i}\ln L_{\nu}}{M_{i}Y_{k}}
\end{gather}$$

Первое тождество Вальда


Приближенно:

| $H_{0}$ | $\ln L_{\nu}$ | $\ln A$   | $\ln B$    |
| ------- | ------------- | --------- | ---------- |
|         |               | $\alpha$  | $1-\alpha$ |
| $H_{1}$ | $\ln L_{\nu}$ | $\ln A$   | $\ln B$    |
|         |               | $1-\beta$ | $\beta$    |

$$\begin{gather}
M_{0}\nu=\frac{\ln A\cdot \alpha+\ln(B)(1-\alpha)}{M_{0}Y_{k}} \\
M_{0} \nu=\frac{\ln A(1-\beta)+\ln B\cdot \beta}{M_{1}Y_{k}} 
\end{gather}$$
Пример:
$$\begin{gather}
X_{1},X_{2},\ldots,\sim E(\lambda) \\
H_{0}:\lambda=\frac{1}{3} \\
H_{1}:\lambda=\frac{5}{11} \\
\alpha=0.05 \\
\beta=0.05 \\
Y_{k}=\ln\left( \frac{\lambda_{1}e^{-\lambda x_{k}}}{\lambda_{0}e^{-\lambda x_{k}} }\right)=\ln\left( \frac{\lambda_{1}}{\lambda_{0}} \right)-(\lambda_{1}-\lambda_{0})X_{k} \\
M_{0}Y_{k}=\ln\left( \frac{\lambda_{1}}{\lambda_{0}} \right)-(\lambda_{1}-\lambda_{0})\cdot \frac{1}{\lambda_{0}}=-0.053 \\
\ln\left( \frac{15}{11} \right)+1-\frac{15}{11} \\
M_{0}\nu=\frac{\ln19\cdot 0.05+\ln\left( \frac{1}{19} \right)0.95}{-0.053}=49.35
\end{gather}$$
Номер 6 домашнего задания
Дисперсия работы критерия?

Центральная теорема второго модуля: критерий Пирсона
Непараметрические критерии
$$x_{1},x_{2},\ldots,x_{n}\sim F(x)$$
Выборка из закона, не зависящего от параметров
$H_{0}:$ Выборка распределена по $F(x)$
$H_{1}:$ выборка распределена не по $F(x)$ 
Критерий Пирсона
$m$ - число группировки

|                          | $[a_{1},b_{1})$                      | $[a_{2},b_{2})$ | $\ldots$ | $[a_{m},b_{m})$ |
| ------------------------ | ------------------------------------ | --------------- | -------- | --------------- |
| <br>эмпирические частоты | $\nu_{1}$                            | $\nu_{2}$       | $\ldots$ | $\nu_{m}$       |
| теоретические частоты    | $\frac{n}{p_{1}}(F(b_{1})-F(a_{1}))$ | $np_{2}$        | $\ldots$ | $np_{m}$        |
$$\begin{gather}
\cup_{i=1}^m [a_{i},b_{i})=\text{область значений }x_{i}
\end{gather}$$
Метрика
$$\begin{gather}
\chi ^2_{B}=\sum_{i=1}^{m} \frac{(\nu_{k}-np_{k})^2}{np_{k}}
\end{gather}$$
Теорема
Если проверяется простая гипотеза, то статистика $\chi^2_{B}$ по распределению 
$$\chi ^2_{B}\underset{ n \to \infty }{ \to} \chi ^2(m-1) $$
там еще d над стрелочкой
зафиксиров над $[a_{i},b_{i})$
$$\begin{gather}
\vec{\nu}=(\nu_{1},\nu_{2},\ldots,\nu_{m})-\text{случайная величина} \\
P(\nu_{1}=n_{1},\nu_{2}=n_{2},\ldots,\nu_{m}=n_{m})=\frac{n!}{n_{1}!n_{2}!\ldots n_{m}!} p_{1}^{n_{1}}p_{2}^{n_{2}}\cdot p_{m}^{n_{m}}
\end{gather}$$
Доказательство:
$$\begin{gather}
(p_{1}+p_{2}+p_{3}+\ldots+p_{m})^n=1 \\
(p_{1}+p_{2}+p_{3}+\ldots+p_{m})^n=\sum_{(n_{1},n_{2},\ldots,n_{m})} \frac{n!}{n_{1}!n_{2}!\ldots n_{m}!} p_{1}^{n_{1}}p_{2}^{n_{2}}\cdot p_{m}^{n_{m}} \\
\text{характеристическая функция }  \\
f_{\vec{\nu}}(\vec{t})=Me^{i\vec{t}^T\vec{\nu}}=M\exp\left( i\sum_{k=1}^{m} t_{k}\nu_{k} \right)
\end{gather}$$
характерстические функции для:
$$\begin{gather}
\sum_{n_{1},n_{2},\ldots,m_{m}} \exp\left( i\sum_{k=1}^{m} t_{k}\nu_{k} \right) \frac{n!}{n_{1}!\ldots n_{m}!} p_{1}^{n_{1}}p_{2}^{n_{2}}\ldots p_{m} ^{n_{m}}=
\end{gather}$$
$$=\sum_{(n_{1},n_{2},\ldots,n_{m})} \frac{n!}{n_{1}!n_{2}!\ldots n_{m}!}(p_{1}e^{it_{1}})^{n_{1}}(p_{2}e^{it_{2}})^{n_{2}}\ldots(p_{m}e^{it_{m}})^{n_{m}}=$$
$$=(p_{1}e^{it_{1}}+p_{2}e^{it_{2}}+\ldots+p_{m}e^{it_{m}})^n=$$
$$=(1+p_{1}(e^{it_{1}}-1)+p_{2}(e^{it_{2}}-1)+\ldots+p_{m}(e^{it_{m}}-1))^n$$
Achtung! потерянный минус
$$\begin{gather}
\vec{\nu}\to \vec{\eta} \\
\eta_{k}=\frac{\nu_{k}-np_{k}}{\sqrt{ np_{k} }} \\
f_{\vec{\eta}}(\vec{t})=M\exp\left( i\sum_{k=1}^{m} t_{k} \frac{\nu_{k}-np_{k}}{\sqrt{ np_{k} }} \right)=\exp-\left( i\sum_{k=1}^{m} t_{k}\sqrt{ np_{k} } \right)M\exp\left( i\sum_{k=1}^{m} \frac{t_{k}}{\sqrt{ np_{k} }}\nu_{k} \right) \\
f_{\vec{\nu}}\left( \frac{t_{1}}{\sqrt{ np_{1} }}, \frac{t_{2}}{\sqrt{ np_{2} }}, \ldots, \frac{t_{m}}{\sqrt{ np_{m} }}  \right) \\
\ln f_{\vec{\eta}}(\vec{t})=i\sum_{k=1}^{m} t_{k}\sqrt{ np_{k} }+n\ln\left( 1+p_{1}\left( e^{i \frac{t_{1}}{\sqrt{ np_{k} }}}-1 \right) +\ldots+p_{m}\left( \exp\left( i \frac{t_{m}}{\sqrt{ np_{m} }} \right)-1 \right)\right)= \\
=i\sum_{k=1}^{m} t_{k}\sqrt{ np_{k} }+n\left( \sum_{k=1}^{n} p_{k}\left( \exp i \frac{t_{k}}{\sqrt{ np_{k} }}-1 \right)-\frac{1}{2}\left( \sum_{k=1}^{n} p_{k}\left( \exp\left( i \frac{t_{k}}{\sqrt{ np_{k} }}\right)-1 \right) \right)^2+o\left( \frac{1}{n} \right) \right)
\end{gather}$$
$$\begin{gather}
=\cancel{ i\sum_{k=1}^{m} t_{k}\sqrt{ np_{k} } }+n\left( \cancel{ \sum_{k=1}^{m} p_{k} \frac{it_{k}}{\sqrt{ np_{k} }} }-\frac{1}{2}\sum_{k=1}^{m} p_{k} \frac{t^2_{k}}{np_{k}} -\frac{1}{2}\left( \sum_{k=1}^{m} p_{k} \frac{it_{k}}{\sqrt{ np_{k} }} \right)^2+o\left( \frac{1}{n} \right) \right)=
\end{gather}$$
$$\begin{gather}
=-\frac{1}{2}\sum_{k=1}^{m} t_{k}^2+\frac{1}{2}\left( \sum_{k=1}^{m} t_{k}\sqrt{ p_{k} }  \right)^2+o(1)=-\frac{1}{2}\vec{t}^T\vec{t}+((Ct)_{1} )^2\\
C=\begin{pmatrix}
\sqrt{ p_{1} } & \sqrt{ p_{2} } & \ldots & \sqrt{ p_{m} } \\
? & ? & \ldots & ? \\
\ldots & \ldots & \ldots & \ldots \\
? & ? & \ldots & ?
\end{pmatrix}-\text{дополняем до ортонормированной матрицы} \\
\vec{\xi}=C\vec{\eta} \\
f_{\vec{\xi}}=M\exp(i\vec{u}^T\vec{\xi})=M\exp(i\vec{u}^TC\vec{\eta})=M\exp(i(C^T\vec{u}^T)^T\vec{\xi})=f_{\vec{\eta}}(C^T \vec{u}) \\
\ln f_{\vec{\xi}}(\vec{u})=\ln f_{\vec{\eta}}(C^T \vec{n})=-\frac{1}{2} (C^T \vec{u})^T C^T \vec{u} + \frac{1}{2}(CC^Tu)_{1}+o(1)=-\frac{1}{2}\vec{u}^T\vec{u}+\frac{1}{2}u^2_{1}+o(1)= \\
=-\frac{1}{2}\sum_{k=2}^{m} u_{k}+o(1)
\end{gather}$$
$$\begin{gather}
f_{\vec{\xi}}(\vec{u})=\exp\left( -\frac{1}{2}\sum_{k=2}^{m}u_{k}^2 +o(1) \right)\Rightarrow \vec{\xi} \text{ нормально распределён в пределе } \\
\Sigma_{\xi}=\begin{pmatrix}
0 & 0 & 0 & \ldots & \ldots \\
0 & 1 & 0 & \ldots & \ldots \\
0 & 0 & 1 & \ldots & \ldots \\
\ldots & \ldots & \ldots & \ldots & 0 \\
\ldots & \ldots & \ldots & \ldots & 1
\end{pmatrix}\Rightarrow \vec{\xi}=\begin{pmatrix}
0 \\
\xi_{2} \\
\xi_{3} \\
\ldots \\
\xi_{m}
\end{pmatrix}
\end{gather}$$
$$\begin{gather}
\chi ^2_{\text{выборочное}}=\sum_{k=1}^{m} \left( \frac{\nu_{k}-np_{k}}{\sqrt{ np_{k} }} \right)^2=\sum_{k=1}^{m} \eta_{k}^2=\sum_{k=1}^{m} \xi_{k}^2\sim \chi ^2(m-1)
\end{gather}$$
$np_k$>=5
Критерий
Критическое множество выборочное $S=(\chi^2_{выбороч}>\chi_{1-\alpha}^2(m-1))$



16/12/2025
Обзор формулы для линейной регрессии
$$\begin{gather}
(X_{1},Y_{1}),(X_{2},Y_{2}),\ldots,(X_{n},Y_{n}) \\
Y_{k}=\sum_{j=0}^{m-1} \beta_{j}\psi_{j}(X_{k})+\underbrace{ \varepsilon_{k} }_{ \text{случайная} } \\
Y=\vec{f}\vec{\beta}+\vec{\varepsilon} \\
M\varepsilon_{k}=0 \\
M\varepsilon_{k}\varepsilon_{j}=0, k\neq j \\
M\varepsilon^2_{k}=\delta^2 \\
\hat{\vec{\beta}}=(\vec{f}^T\vec{f})^{-1}\vec{f}^TY-\text{оценка??? $\beta$}
\end{gather}$$
$$\begin{gather}
M\hat{\vec{\beta}}=\vec{\beta} \\
\Sigma  \hat{\vec{\beta}}=\delta^2(\vec{f}^T\vec{f})^{-1}-\text{дисперсионная матрица Фишера?} \\
?=F  \hat{\vec{\beta}} \\
\hat{\delta^2}=\frac{1}{n-m} \sum_{k=1}^{n} \left( Y_{k}-\sum_{j=?}^{?} ???\ldots??? \right)
\end{gather}$$
тождество 
$$\begin{gather}
\frac{1}{\delta^2}\underbrace{ (y-f\beta)^{T}(y-f\beta) }_{ \varepsilon^T\varepsilon }=\frac{1}{\delta^2}(Y-\hat{Y})(Y-\hat{Y})+\frac{1}{\delta^2}\underbrace{ (\hat{Y}-f\beta)^T(\hat{Y}-f\beta) }_{ (\hat{\beta}-\beta)^Tf^Tf(\hat{\beta}-\beta) }
\end{gather}$$
Доверительные интервалы для параметров регрессии $\beta$ и $\delta$
1. $\frac{n-m}{\delta^2} \hat{\delta^2}\sim \chi(n-m)\Leftrightarrow \frac{1}{\delta^2}(Y-\hat{Y})^T(Y-\hat{Y})\sim \chi^2(n-m)$
$$\begin{gather}
P\left( \chi^2_{\frac{\delta}{2}}(n-m)< \frac{1}{\delta^2}(Y-\hat{Y})^T(Y-\hat{Y})<\chi^2_{1-\frac{d}{2}}(n-m) \right)=1-d
\end{gather}$$
Доверительный интервал для $\delta$:
$$\begin{gather}
\frac{(Y-\hat{Y})^T(Y-\hat{Y})}{\chi^2_{1-\frac{d}{2}}(n-m)}<\delta^2< \frac{(Y-\hat{Y})^T(Y-\hat{Y})}{\chi^2_{\frac{d}{2}}(n-m)}
\end{gather}$$
$Y=\beta_{0}+\beta_{1}x+\beta_{2}x^2$
Доверительные интервалы для $\beta_{i}$
	а) $\delta$ известно $\Rightarrow \vec{\beta}=(f^Tf)^{-1}f^TY$
	$$\begin{gather}
Y=\vec{f}\vec{\beta}+\varepsilon \sim N(?, ?)\Rightarrow Y\sim N(\vec{f}\vec{\beta},\delta^2E_{n}) \\
\vec{\beta}=(f^Tf)^{-1}f^TY\sim N(\vec{\beta},\delta^2(f^Tf)^{-1} ) \\
\frac{\hat{\beta}_{i}-\beta_{i}}{\delta \sqrt{ \tau_{ii} }}\sim N(0,1)\to \hat{\beta}_{i-u_{1-\frac{d}{2}}}\delta \sqrt{ \tau_{ii} }<\beta_{i}<\hat{\beta}_{i}+u_{1-\frac{d}{2}}\delta \sqrt{ \tau_{ii} }
\end{gather}$$
б) $\delta$ лещвеп???  может быть "неизвестно"?
$$\begin{gather}
\frac{\hat{\beta}_{i}-\beta_{i}}{\hat{\delta}\sqrt{ \tau_{ii} }}=\frac{\frac{\hat{\beta}_{i}-\beta_{i}}{\delta \sqrt{ \tau_{ii} }}}{\frac{\hat{\delta}}{\delta}-\sqrt{ \frac{(n-m)\hat{\delta}^2}{\delta^2} }\sim \chi^2(n-m)}\sim t(n-m) \\
\hat{\beta}_{i}-t_{1-\frac{d}{2}}(n-m)\hat{\delta}\sqrt{ \tau_{ii} }<\beta_{i}<\hat{\beta}_{i}+t_{1-\frac{d}{2}}(n-m)\hat{\delta}\sqrt{ \tau_{ii} }
\end{gather}$$

Проверка гипотез о параметрах $\vec{\beta}$
1. $\begin{matrix}H_{0_{i}}:\beta_{i}=0 \\ H_{1i}:\beta_{i}\neq0\end{matrix}$
$$\begin{gather}
S=\{ \lvert \hat{\beta}_{i} \rvert >C \} \\
\alpha=P\{ \lvert \hat{\beta}_{i} \rvert >C \left.  \right\rvert{\beta_{i}=0}  \}=1-P(\lvert \hat{\beta}_{i} \rvert >C \vert \beta_{i}=0 )\Rightarrow  \\
C=t_{1-\frac{d}{2}}(n-m) \hat{\delta} \sqrt{ \tau_{ii} } 
\end{gather}$$
2. $\psi_{0}(x_{k})=1$
$$\begin{gather}
H_{0}:\beta_{1}=\beta_{2}=\ldots=\beta_{m-1}=0  \\
f=\begin{pmatrix}
1 & \ldots & \ldots \\
1 & \ldots & \ldots \\
1 & \ldots & \ldots
\end{pmatrix}\\
H_{1}: \exists\beta_{i}\neq 0 \\
Q_{l}=(Y-\hat{Y})^T(Y-\hat{Y}) \\
\frac{Q_{l}}{\delta^2}\sim \chi^2(n-m) \\
Q_{f}=(\hat{Y}-\vec{Y}\cdot \vec{I}) \\
I=\begin{pmatrix}
1 \\
1 \\
1 \\
\ldots \\
1
\end{pmatrix}, \hat{Y}=\frac{1}{n}\sum_{k=1}^{n} Y_{k} \\
(\hat{Y}-\vec{f}\vec{\beta})^T(\vec{Y}-\vec{f}\vec{\beta}) \frac{1}{\delta^2}\sim \chi^2(m)
\end{gather}$$
$$\begin{gather}
H_{0}\to \vec{\beta}=\begin{pmatrix}
\beta_{0} \\
0 \\
\ldots \\
0
\end{pmatrix}\to \vec{f}\vec{\beta}=\begin{pmatrix}
\beta_{0} \\
\beta_{0} \\
\ldots \\
\beta_{0}
\end{pmatrix}=\beta_{0}I
\end{gather}$$
$$\begin{gather}
\frac{Q_{f}}{\delta^2}\sim \chi^2(m-1)
\end{gather}$$
$m-1$ связано с тем, что из-за оценки число степеней свободы уменьшилось на 1.
Оценка: $\hat{\beta}_{0}=\vec{Y}$
$$\begin{gather}
\alpha=\left\{   \frac{Q_{f}}{Q_{l}}>C_{1} \vert H_{i} \right\}=\left\{  \frac{Q_{f}(n-m)}{(m-1)Q_{l}} \right\} \sim F(m-1,n-m)\\
\end{gather}$$
Критерий: $\frac{Q_{f}}{m-1} \frac{n-m}{Q_{l}}>F_{1-\alpha}(m-1,n-m)$
Подгонка модели
Вычисляем $\beta$. Самые маленькие $\beta$ проверяем на $t$-критерий Стьюдента. Оставлять член регрессии или не оставлять? 

Проверка адекватности регрессионной модели
(Если доступны повторные наблюдения). 
$$\begin{gather}
\vec{X}_{k}\to (Y_{k}^{(1)},\ldots,Y_{k}^{(r_{k})})-r_{k}\text{ откликов} \\
\to \overline{Y}_{k}=\frac{1}{r_{k}}\sum_{j=1}^{r_{k}} Y_{k}^{(j)} \\
Q_{n}=\sum_{k=1}^{n} (\hat{Y}_{k}-\overline{Y}_{k})^2r_{k}-\text{мера адекватности регрессионной модели} \\
\frac{Q_{n}}{\delta^2}\sim \chi^2(n-m) \\
Q_{p}=\sum_{k=1}^{n} \sum_{j=1}^{2k} (Y^{(j)}_{k}-\overline{Y}_{k})^2-\text{сумма квадратов отклонений} \\
\frac{Q_{p}}{\delta^2}\sim \chi^2\left( \sum_{k=1}^{n} (r_{k}-1) \right)
\end{gather}$$
Гипотеза $H_{0}:MY=\vec{f}\vec{\beta}$
$$\begin{gather}
Y=\vec{f}\vec{\beta}+\vec{\varepsilon} \\
H_{1}:MY\neq \vec{f}\vec{\beta} \\
\frac{Q_{n}}{n-m} \frac{\sum_{k=1}^{n} (r_{k}-1)}{Q_{p}}\sim F\left( n-m,\sum_{k=1}^{n} (r_{k}-1) \right)
\end{gather}$$
Критерий:
$$\begin{gather}
\frac{Q_{n}}{n-m} \frac{\sum_{k=1}^{n} (r_{k}-1)}{Q_{p}}>F_{1-\alpha}\left( n-m,\sum_{k=1}^{n} (r_{k}-1) \right)\to H_{0}\text{ отклоняем}
\end{gather}$$

Боровков


Пример
$$\begin{gather}
P_{1}= a_{0}+a_{1}(E-E_{0})\\
\chi^2=\frac{Y_{k}-\beta^T_{?}-\hat{\beta}X_{k}}{\sum_{k=1}^{n} \delta_{k}^2}\sim \chi^2(n-m)=\chi^2(20-2)=\chi^2(18) \\
\chi^2_{B}=36.8 \\
\chi^2_{0.99}=(18)=38,8\Rightarrow H_{1} \text{ отклонить} \\
y=\frac{1}{x^2+a^2}
\end{gather}$$
фимезон распадается на пару бимезонов


Проблемы
1. Мультиколлинеарность:  может быть плохо обусловленной. 
$$\begin{gather}
\psi_{0},\psi_{1},\ldots,\psi_{m-1}-\text{ почти линейно зависимы}\Rightarrow  \\
f=\begin{pmatrix}
\psi_{0}(\vec{X}_{1}) & \psi_{1}(X_{1}) & \ldots & \psi_{m-1}(X_{1}) \\
\ldots & \ldots & \ldots & \ldots \\
\psi_{1}(X_{n}) & \ldots & \ldots & \psi_{m-1}(X_{n})
\end{pmatrix} \to f^Tf-\text{вырожденная} 
\end{gather}$$
Пример:
Рассматривается модель
$$\begin{gather}
\hat{g}=\hat{\beta}_{1}x_{1}+\hat{\beta}_{2}x_{2}+\hat{\beta}_{3}x_{3} +\hat{\beta}_{4}x_{4}  \\
\hat{y}=(\hat{\beta}_{1}+a)x_{1}+(\hat{\beta}_{2}-a)x_{2}+(\hat{\beta}_{3}-a)x_{3}+\hat{\beta}_{4}x_{4} \\
\hat{y}=\hat{\beta}_{1}x_{1}+\hat{\beta}_{2}x_{2}+\hat{\beta}_{3}x_{3}+\hat{\beta}_{4}x_{4}+\cancelto{ 0 }{ a(x_{1}-x_{2}-x_{3}) }
\end{gather}$$
Регуляризация Тихонова
$$\begin{gather}
f^Tf+\alpha E \\
\hat{\beta}=(\vec{f}^T\vec{f}+\alpha E)^{-1}\vec{f}^TX
\end{gather}$$
отход от темы: Белофелов?? Метод главных компонент...
Конец отхода от темы
Есть ещё методы борьбы с мультиколлинеарностью
$$Y_{k}=\varphi (X_{k})+\varepsilon$$
$$X_{k}\sim N(?,?)\to \varphi (X_{k})=\sum_{j=0}^{\ldots} a_{j}\psi_{j}(X_{?})$$

Полиномы Эрмита
$$\begin{gather}
1, x, \frac{x^2-1}{\sqrt{ 2 }} \\
\frac{1}{\sqrt{ 2\pi }} \int _{-\infty}^{+\infty} He_{n}(x)He_{m}(x)e^{-x^2/2}dx=\delta^{n\neq m}
\end{gather}$$
Равномерный закон распределения -> полиномы Лежандра
























